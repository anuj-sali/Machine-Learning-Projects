DL FOR DEVELOPERS; DAY 1: ADVANCED SESSION:


1.THEORY: MATHS, THEORY
2. CODE ASPECT: KERAS, TF, PYTORCH


PREREQUISITES:
1.PYTHON & ITS LIBRARIES.
2.ML ALGOS. [EG. LOGISTIC REGRESSION]

FORMULATION OF LOG REG:
w,b--> trained wts
OUTPUT = sigmoid (wT.x + b)    x,w---> vectors, b--> scalar
			       x= [x1,x2,x3..xd], w =[w1,w2,w3..wd]

O = sigmoid(w1x1+w2x2 +...wdxd + b)
O = sigmoid(summation i = 1 to d (xi*wi) +b))


HISTORY OF DL:
1957: PERCEPTRON [artificial single neuron system]

x1,x2,x3,x4, xd, b  [w1,w2,w3,...wd],1 ---> blackbox ---> output

F--> activation function

O = F(wT.x +b )
O = F(summation i = 1 to d (xi*wi) + b))
F--> ? heaviside function / thresholding function

F(x):
wT.x + b > 0 --> 1
wT.x + b<= 0 --> 0

different activation functions:

characteristics of activation function:
1. easily differentiable.
2. they should be differentiable @ all points

Gradient descent:
2012: IMAGENET --> CNN DL MODEL  [object detection]
1. Heaviside Step Function/ Thresholding Function--> perceptron
2. Sigmoid function ---> logistic regression
3. linear unit: F(x) = x ---> linear regression
4. RelU--> rectified linear unit

f(x): x ; x>0
      0 ; x<0

5. Leaky relu:

f(x): x ; x>0
      a ; x<0   a=0.01

6. tanh
.
.
.
.



Multi-Layered Perceptron [MLP]: 1-3 hidden layers
DEEP LEARNING MODEL: >3 hidden layers


classical machine learning [optimization]
-regularization.
-solvers in sklearn

classical ML ---> DL

1.lots of data 
2.computation power high 

Optimizers: 
finding min/max value

f(x)
1st derivate of function = 0 
f'(x) = 0  --> position of x at which min/max value occurs
f''(x) < 0 --> 
f''(x) > 0 --> 


gradient descent: iterative algorithm: you always get global min*

1. Stochastic Gradient Descent optimizer: [SGD]

random no: x0
iterations start:
x1 = x0 - (eta)* (df/dx)(at x0)   [update function]

xj+1 = xj - (eta)* (df/dx)(at xj)   [update function]

xj+1 == xj
stop iteration: declare min = xj

2.SGD + MOMENTUM:

3. Nesterov Accelerated Gradient DEscent (NAG)

4. RMSProp:

5.Adadelta:

5.Adagrad.

6.Adam.

DEMO: ANN
MULTI-CLASS CLASSIFICATION:
TARGET: OLD, MIDDLE, YOUNG
GIVEN A RGB IMAGE

IMAGE--> (h,wt,depth)  RGB image--> depth = 3
		       B/W --> depth = 1


PREPROCESSING OF DATA --> ANY ML MODEL
(32,32,3)
INVARIABILITY.



No of features of a single RGB image = ht*wt*depth
each pixel --> 1 feature


no of nodes in O/P layer = no of classes = 3

MLP/DL model

input --> hidden layers--> output (classfier --> softmax /regressor)
softmax --> log loss--> logistic regression



1 epoch : going thr entire training data once.


parameters of data : wts, bias
parameter of algorithm: learning rate,optimizers, epoch, losses
hypertune the hyperparameter --> boost the performance of algo.

REGULARIZATION:
1. DROPOUT:
2. LO,L1 & L2 

TRAIN & TEST DATA SET: TEST DATA AT LAST

TRAIN DATA: TRAIN & VALIDATION DATA 















































